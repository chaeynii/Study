{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd46873e",
   "metadata": {},
   "source": [
    "## PySpark 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f17f3166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==3.5.6 in c:\\users\\chaeyni\\documents\\study\\spark\\venv\\lib\\site-packages (3.5.6)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\chaeyni\\documents\\study\\spark\\venv\\lib\\site-packages (from pyspark==3.5.6) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: wget in c:\\users\\chaeyni\\documents\\study\\spark\\venv\\lib\\site-packages (3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark==3.5.6\n",
    "%pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e22a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, wget\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d154c30",
   "metadata": {},
   "source": [
    "**Spark Session**: SparkSession은 Spark 2.0부터 엔트리 포인트로 사용된다. SparkSession을 이용해 RDD, 데이터 프레임등을 만든다. SparkSession은 SparkSession.builder를 호출하여 생성하며 다양한 함수들을 통해 세부 설정이 가능하다\n",
    "\n",
    "- local[*] Spark이 하나의 JVM으로 동작하고 그 안에 컴퓨터의 코어 수 만큼의 스레드가 Executor로 동작한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "372e54a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 로컬 호스트 IP 구하기\n",
    "host_ip = \"127.0.0.1\"  # 또는 socket.gethostbyname(socket.gethostname())\n",
    "\n",
    "# 2) 환경변수 지정\n",
    "os.environ['SPARK_LOCAL_IP']           = host_ip\n",
    "os.environ['PYSPARK_PYTHON']           = os.sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON']    = os.sys.executable\n",
    "\n",
    "# 3) SparkSession 생성 시 config 추가\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('PySpark Tutorial')\\\n",
    "    .config(\"spark.driver.host\", host_ip) \\\n",
    "    .config(\"spark.driver.bindAddress\", host_ip) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "819cf96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver Host: 127.0.0.1\n",
      "Bind Address: 127.0.0.1\n",
      "SPARK_LOCAL_IP   : 127.0.0.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Driver Host:\", spark.sparkContext.getConf().get(\"spark.driver.host\"))\n",
    "print(\"Bind Address:\", spark.sparkContext.getConf().get(\"spark.driver.bindAddress\"))\n",
    "print(\"SPARK_LOCAL_IP   :\", os.environ.get(\"SPARK_LOCAL_IP\", None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac694aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.driver.host conf : 127.0.0.1\n",
      "InetAddress.getLocalHost(): 192.168.45.190\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "# 1) Spark 설정에 남아 있는 드라이버 호스트\n",
    "host_conf = spark.sparkContext.getConf().get(\"spark.driver.host\", None)\n",
    "\n",
    "# 2) Java가 기본으로 뽑아낸 로컬 호스트 IP\n",
    "local_ip = socket.gethostbyname(socket.gethostname())\n",
    "\n",
    "print(\"spark.driver.host conf :\", host_conf)\n",
    "print(\"InetAddress.getLocalHost():\", local_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "637105ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://127.0.0.1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.6</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark Tutorial</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x16e75210940>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f80f7713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture: 64bit\n",
      "Processor: Intel64 Family 6 Model 189 Stepping 1, GenuineIntel\n",
      "Cores (Physical): 8\n",
      "Threads (Logical): 8\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import psutil\n",
    "\n",
    "print(\"Architecture:\", platform.architecture()[0])\n",
    "print(\"Processor:\", platform.processor())\n",
    "print(\"Cores (Physical):\", psutil.cpu_count(logical=False))\n",
    "print(\"Threads (Logical):\", psutil.cpu_count(logical=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5715ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TotalVisibleMemorySize\n",
      "----------------------\n",
      "              33070452\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!powershell \"Get-CimInstance -ClassName Win32_OperatingSystem | Select-Object TotalVisibleMemorySize\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6069f443",
   "metadata": {},
   "source": [
    "## Python <> RDD <> DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1369d82a",
   "metadata": {},
   "source": [
    "#### Python 객체를 RDD로 변환해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312bee22",
   "metadata": {},
   "source": [
    "##### 1> Python 리스트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76ef2f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list_json = [ '{\"name\": \"chaeyeon\"}',\n",
    "                  '{\"name\": \"yeonsu\"}',\n",
    "                  '{\"name\": \"somsom\"}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47c2ab14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"chaeyeon\"}\n",
      "{\"name\": \"yeonsu\"}\n",
      "{\"name\": \"somsom\"}\n"
     ]
    }
   ],
   "source": [
    "for n in name_list_json:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf3b121",
   "metadata": {},
   "source": [
    "##### 2> 파이썬 리스트를 RDD로 변환\n",
    "- RDD로 변환되는 순간 Spark 클러스터들의 서버들에 데이터가 나눠 저장됨(파티션)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4992010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize(name_list_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a1d0549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75688eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edc316f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "parsed_rdd = rdd.map(lambda el:json.loads(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7eab25a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[2] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04973219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'chaeyeon'}, {'name': 'yeonsu'}, {'name': 'somsom'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62c87963",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_name_rdd = rdd.map(lambda el:json.loads(el)[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba633533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chaeyeon', 'yeonsu', 'somsom']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_name_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b472a94",
   "metadata": {},
   "source": [
    "##### 파이썬 리스트를 데이터프레임으로 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53c1bf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "df = spark.createDataFrame(name_list_json, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30fa2979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "160de890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07392df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='{\"name\": \"chaeyeon\"}'),\n",
       " Row(value='{\"name\": \"yeonsu\"}'),\n",
       " Row(value='{\"name\": \"somsom\"}')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('*').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ce7687",
   "metadata": {},
   "source": [
    "RDD를 DataFrame으로 변환해보는 예제: 앞서 parsed_rdd를 DataFrame으로 변환해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d2328b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed_rdd = parsed_rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19a048a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parsed_rdd.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d866a808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='chaeyeon'), Row(name='yeonsu'), Row(name='somsom')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_parsed_rdd.select('name').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a4b249",
   "metadata": {},
   "source": [
    "### Spark 데이터프레임으로 로드해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af191bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'data/name_gender.csv'\n",
    "download_path = 'https://s3-geospatial.s3-us-west-2.amazonaws.com/name_gender.csv'\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    wget.download(download_path, out=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfa6a7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "name_gender_csv = save_path\n",
    "df = spark.read.csv(name_gender_csv)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3b6a147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", True).csv(name_gender_csv)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6557fa02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|      name|gender|\n",
      "+----------+------+\n",
      "|  Adaleigh|     F|\n",
      "|     Amryn|Unisex|\n",
      "|    Apurva|Unisex|\n",
      "|    Aryion|     M|\n",
      "|    Alixia|     F|\n",
      "|Alyssarose|     F|\n",
      "|    Arvell|     M|\n",
      "|     Aibel|     M|\n",
      "|   Atiyyah|     F|\n",
      "|     Adlie|     F|\n",
      "|    Anyely|     F|\n",
      "|    Aamoni|     F|\n",
      "|     Ahman|     M|\n",
      "|    Arlane|     F|\n",
      "|   Armoney|     F|\n",
      "|   Atzhiry|     F|\n",
      "| Antonette|     F|\n",
      "|   Akeelah|     F|\n",
      "| Abdikadir|     M|\n",
      "|    Arinze|     M|\n",
      "+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0babc92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Adaleigh', gender='F'),\n",
       " Row(name='Amryn', gender='Unisex'),\n",
       " Row(name='Apurva', gender='Unisex'),\n",
       " Row(name='Aryion', gender='M'),\n",
       " Row(name='Alixia', gender='F')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9d79e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(gender='F', count=65),\n",
       " Row(gender='M', count=28),\n",
       " Row(gender='Unisex', count=7)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"gender\"]).count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21e06bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3e20af",
   "metadata": {},
   "source": [
    "데이터프레임을 테이블뷰로 만들어서 SparkSQL로 처리해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3e9bbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"namegender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c944dd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "namegender_group_df = spark.sql(\"SELECT gender, count(1) FROM namegender GROUP BY 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98c67c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(gender='F', count(1)=65),\n",
       " Row(gender='M', count(1)=28),\n",
       " Row(gender='Unisex', count(1)=7)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "namegender_group_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "521cef0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='namegender', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae467a4",
   "metadata": {},
   "source": [
    "Partition의 수 계산해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "280b0cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "namegender_group_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ee87380",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_namegender_group_df = namegender_group_df.repartition(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c747342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_namegender_group_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3510472",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
