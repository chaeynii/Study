{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9151b08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==3.5.3 in c:\\users\\chaeyni\\documents\\study\\spark\\venv\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\chaeyni\\documents\\study\\spark\\venv\\lib\\site-packages (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: wget in c:\\users\\chaeyni\\documents\\study\\spark\\venv\\lib\\site-packages (3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark==3.5.3 py4j==0.10.9.7\n",
    "%pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f91d76ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os, sys, wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "480b64f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 로컬 호스트 IP 구하기\n",
    "host_ip = \"127.0.0.1\"  # 또는 socket.gethostbyname(socket.gethostname())\n",
    "\n",
    "# 2) 환경변수 지정\n",
    "os.environ['SPARK_LOCAL_IP']           = host_ip\n",
    "os.environ['PYSPARK_PYTHON']           = os.sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON']    = os.sys.executable\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:\\\\Hadoop\\\\hadoop-3.3.6\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d033d1",
   "metadata": {},
   "source": [
    "PySpark Py4JJavaError : write.csv 오류 해결\n",
    "\n",
    "> python 3.11.9 버전 호환\n",
    "https://www.reddit.com/r/apachespark/comments/1evu4zz/error_with_pyspark_and_py4j/\n",
    "\n",
    "> hadoop.dll 파일 설치\n",
    "https://www.reddit.com/r/apachespark/comments/18ftcgs/unable_to_write_dataframe_to_files_using_pyspark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb5a4e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"PySpark DataFrame #3\")\n",
    "conf.set(\"spark.master\", \"local[*]\")\n",
    "conf.set(\"spark.driver.host\", host_ip)\n",
    "conf.set(\"spark.driver.bindAddress\", host_ip)\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .config(conf=conf)\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "429a554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'data/transfer_cost.txt'\n",
    "download_path = 'https://s3-geospatial.s3-us-west-2.amazonaws.com/transfer_cost.txt'\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    wget.download(download_path, out=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "386258b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active code page: 65001\n",
      " Volume in drive C has no label.\n",
      " Volume Serial Number is BC10-1C56\n",
      "\n",
      " Directory of c:\\Users\\chaeyni\\Documents\\study\\spark\\programming-pyspark\\chapter2\\data\n",
      "\n",
      "2025-07-28  오후 10:01    <DIR>          .\n",
      "2025-07-27  오후 11:53    <DIR>          ..\n",
      "2025-07-27  오후 05:13            62,728 1800.csv\n",
      "2025-07-27  오후 10:27           146,855 customer-orders.csv\n",
      "2025-07-28  오후 10:01    <DIR>          extracted.csv\n",
      "2025-07-27  오후 05:04               997 name_gender.csv\n",
      "2025-07-27  오후 11:37           286,779 transfer_cost.txt\n",
      "               4 File(s)        497,359 bytes\n",
      "               3 Dir(s)  614,474,407,936 bytes free\n"
     ]
    }
   ],
   "source": [
    "!chcp 65001\n",
    "!dir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5dcef504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On 2021-01-04 the cost per ton from 85001 to 85002 is $28.32 at ABC Hauling\n",
      "On 2021-01-04 the cost per ton from 85001 to 85004 is $25.68 at ABC Hauling\n",
      "On 2021-01-04 the cost per ton from 85001 to 85007 is 19.86 at ABC Hauling\n",
      "On 2021-01-04 the cost per ton from 85001 to 85007 is 20.52 at Haul Today\n",
      "On 2021-01-04 the cost per ton from 85001 to 85010 is 20.72 at Haul Today\n"
     ]
    }
   ],
   "source": [
    "!powershell -Command \"Get-Content data/transfer_cost.txt | Select-Object -First 5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d49fd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "transfer_cost_txt = save_path\n",
    "\n",
    "schema = StructType([ StructField(\"text\", StringType(), True)])\n",
    "transfer_cost_df = spark.read.schema(schema).text(transfer_cost_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90d8405b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------+\n",
      "|text                                                                       |\n",
      "+---------------------------------------------------------------------------+\n",
      "|On 2021-01-04 the cost per ton from 85001 to 85002 is $28.32 at ABC Hauling|\n",
      "|On 2021-01-04 the cost per ton from 85001 to 85004 is $25.68 at ABC Hauling|\n",
      "|On 2021-01-04 the cost per ton from 85001 to 85007 is 19.86 at ABC Hauling |\n",
      "|On 2021-01-04 the cost per ton from 85001 to 85007 is 20.52 at Haul Today  |\n",
      "|On 2021-01-04 the cost per ton from 85001 to 85010 is 20.72 at Haul Today  |\n",
      "|On 2021-01-04 the cost per ton from 85001 to 85012 is $18.98 at ABC Hauling|\n",
      "|On 2021-01-04 the cost per ton from 85001 to 85013 is 26.64 at Haul Today  |\n",
      "|On 2021-01-04 the cost per ton from 85001 to 85020 is 26.34 at ABC Hauling |\n",
      "|On 2021-01-04 the cost per ton from 85001 to 85021 is $20.15 at ABC Hauling|\n",
      "|On 2021-01-04 the cost per ton from 85002 to 85001 is 21.57 at Haul Today  |\n",
      "|On 2021-01-04 the cost per ton from 85002 to 85004 is 21.40 at Haul Today  |\n",
      "|On 2021-01-04 the cost per ton from 85002 to 85007 is 25.93 at Haul Today  |\n",
      "|On 2021-01-04 the cost per ton from 85002 to 85010 is 19.80 at Haul Today  |\n",
      "|On 2021-01-04 the cost per ton from 85002 to 85012 is 21.66 at Haul Today  |\n",
      "|On 2021-01-04 the cost per ton from 85002 to 85013 is $25.90 at Haul Today |\n",
      "|On 2021-01-04 the cost per ton from 85002 to 85020 is 19.15 at ABC Hauling |\n",
      "|On 2021-01-04 the cost per ton from 85002 to 85021 is $27.13 at Haul Today |\n",
      "|On 2021-01-04 the cost per ton from 85004 to 85001 is 23.88 at Haul Today  |\n",
      "|On 2021-01-04 the cost per ton from 85004 to 85002 is 26.40 at ABC Hauling |\n",
      "|On 2021-01-04 the cost per ton from 85004 to 85007 is 26.03 at ABC Hauling |\n",
      "+---------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transfer_cost_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90758882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "regex_str = r'On (\\S+) the cost per ton from (\\d+) to (\\d+) is (\\S+) at (.*)'\n",
    "\n",
    "df_with_new_columns = transfer_cost_df\\\n",
    "                        .withColumn('week', regexp_extract('text', regex_str, 1))\\\n",
    "                        .withColumn('departure_zipcode', regexp_extract(column('text'), regex_str, 2))\\\n",
    "                        .withColumn('arrival_zipcode', regexp_extract(transfer_cost_df.text, regex_str, 3))\\\n",
    "                        .withColumn('cost', regexp_extract(col('text'), regex_str, 4))\\\n",
    "                        .withColumn('vendor', regexp_extract(col('text'), regex_str, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6958649c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- week: string (nullable = true)\n",
      " |-- departure_zipcode: string (nullable = true)\n",
      " |-- arrival_zipcode: string (nullable = true)\n",
      " |-- cost: string (nullable = true)\n",
      " |-- vendor: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_new_columns.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dc6f0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = df_with_new_columns.drop(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c2f093a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[week: string, departure_zipcode: string, arrival_zipcode: string, cost: string, vendor: string]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c396b612",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"data/extracted.csv\"\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    final_df.write.csv(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35304223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is BC10-1C56\n",
      "\n",
      " Directory of c:\\Users\\chaeyni\\Documents\\study\\spark\\programming-pyspark\\chapter2\\data\n",
      "\n",
      "2025-08-02  오후 03:07    <DIR>          .\n",
      "2025-07-27  오후 11:53    <DIR>          ..\n",
      "2025-07-27  오후 05:13            62,728 1800.csv\n",
      "2025-07-27  오후 10:27           146,855 customer-orders.csv\n",
      "2025-08-02  오후 03:07    <DIR>          extracted.csv\n",
      "2025-07-27  오후 05:04               997 name_gender.csv\n",
      "2025-07-27  오후 11:37           286,779 transfer_cost.txt\n",
      "               4 File(s)        497,359 bytes\n",
      "               3 Dir(s)  613,761,634,304 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2eb7cb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is BC10-1C56\n",
      "\n",
      " Directory of c:\\Users\\chaeyni\\Documents\\study\\spark\\programming-pyspark\\chapter2\\data\\extracted.csv\n",
      "\n",
      "2025-08-02  오후 03:07    <DIR>          .\n",
      "2025-08-02  오후 03:07    <DIR>          ..\n",
      "2025-08-02  오후 03:07             1,264 .part-00000-bdff1522-8c10-4aeb-b480-e546020362f6-c000.csv.crc\n",
      "2025-08-02  오후 03:07                 8 ._SUCCESS.crc\n",
      "2025-08-02  오후 03:07           160,257 part-00000-bdff1522-8c10-4aeb-b480-e546020362f6-c000.csv\n",
      "2025-08-02  오후 03:07                 0 _SUCCESS\n",
      "               4 File(s)        161,529 bytes\n",
      "               2 Dir(s)  613,759,090,688 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir data\\extracted.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6ba44005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-04,85001,85002,$28.32,ABC Hauling\n",
      "2021-01-04,85001,85004,$25.68,ABC Hauling\n",
      "2021-01-04,85001,85007,19.86,ABC Hauling\n",
      "2021-01-04,85001,85007,20.52,Haul Today\n",
      "2021-01-04,85001,85010,20.72,Haul Today\n"
     ]
    }
   ],
   "source": [
    "!powershell -Command \"Get-Content data/extracted.csv/part-00000-bdff1522-8c10-4aeb-b480-e546020362f6-c000.csv | Select-Object -First 5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4be2a22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path_json = \"data/extracted.json\"\n",
    "\n",
    "if not os.path.exists(save_path_json):\n",
    "    final_df.write.format(\"json\").save(save_path_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4b22afb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is BC10-1C56\n",
      "\n",
      " Directory of c:\\Users\\chaeyni\\Documents\\study\\spark\\programming-pyspark\\chapter2\\data\\extracted.json\n",
      "\n",
      "2025-08-02  오후 03:29    <DIR>          .\n",
      "2025-08-02  오후 03:29    <DIR>          ..\n",
      "2025-08-02  오후 03:29             3,420 .part-00000-aa8cfc40-7d09-4b3d-82c7-fe08074e94b4-c000.json.crc\n",
      "2025-08-02  오후 03:29                 8 ._SUCCESS.crc\n",
      "2025-08-02  오후 03:29           436,305 part-00000-aa8cfc40-7d09-4b3d-82c7-fe08074e94b4-c000.json\n",
      "2025-08-02  오후 03:29                 0 _SUCCESS\n",
      "               4 File(s)        439,733 bytes\n",
      "               2 Dir(s)  613,751,742,464 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir data\\extracted.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5801e49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"week\":\"2021-01-04\",\"departure_zipcode\":\"85001\",\"arrival_zipcode\":\"85002\",\"cost\":\"$28.32\",\"vendor\":\"ABC Hauling\"}\n"
     ]
    }
   ],
   "source": [
    "!powershell -Command \"Get-Content data/extracted.json/part-00000-aa8cfc40-7d09-4b3d-82c7-fe08074e94b4-c000.json | Select-Object -First 1\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
